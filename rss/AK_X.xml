<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title><![CDATA[Twitter @AK]]></title><link/> https://twitter.com/_akhaliq <atom:link href="https://rsshub-yuee.vercel.app/twitter/user/_akhaliq/readable=1&amp;authorNameBold=1&amp;showAuthorInTitle=1&amp;showAuthorInDesc=1&amp;showQuotedAuthorAvatarInDesc=1&amp;showAuthorAvatarInDesc=1&amp;showEmojiForRetweetAndReply=1&amp;showRetweetTextInTitle=0&amp;addLinkForPics=1&amp;showTimestampInDescription=1&amp;showQuotedInTitle=1&amp;heightOfPics=150&amp;excludeReplies=1?key=yuezhu" rel="self" type="application/rss+xml"></atom:link><description><![CDATA[AI research paper tweets, ML @Gradio (acq. by @HuggingFace 🤗)

dm for promo - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description><generator> RS集线器</generator><webmaster>i@diygod.me (DIYgod)</webmaster><language> zh-cn</language><image/><url> https://pbs.twimg.com/profile_images/1451191636810092553/kpM5Fe12.jpg</url><title><![CDATA[Twitter @AK]]></title><link/> https://twitter.com/_akhaliq<lastbuilddate> 2023 年 8 月 15 日星期二 06:02:17 GMT</lastbuilddate><ttl> 10 </ttl><item><title><![CDATA[AK: Platypus: Quick, Cheap, and Powerful Refinement of LLMs paper page: https://huggingface.co/papers/2308.07317 present Platypus, a family of fine-tu...]]></title><description><![CDATA[<img width="0" height="0" hidden="true" src="https://pbs.twimg.com/media/F3jPI-9aoAEOhV9?format=jpg&amp;name=orig" referrerpolicy="no-referrer"> <a href="https://twitter.com/_akhaliq" target="_blank" rel="noopener noreferrer"><img width="48" height="48" src="https://pbs.twimg.com/profile_images/1451191636810092553/kpM5Fe12_normal.jpg" hspace="8" vspace="8" align="left" referrerpolicy="no-referrer"> <strong>AK</strong></a> ：Platypus：快速、廉价且强大的法学硕士改进<br><br>纸质页面：https://huggingface.co/papers/2308.07317<br><br>推出 Platypus，这是一个经过微调和合并的大型语言模型 (LLM) 家族，它实现了最强的性能，目前在 HuggingFace 的 Open LLM 中排名第一……<br clear="both"><div style="clear: both"></div> <a href="https://pbs.twimg.com/media/F3jPI-9aoAEOhV9?format=jpg&amp;name=orig" target="_blank" rel="noopener noreferrer"><img height="150" style="height: 150px;" hspace="4" vspace="8" src="https://pbs.twimg.com/media/F3jPI-9aoAEOhV9?format=jpg&amp;name=orig" referrerpolicy="no-referrer"></a><br clear="both"><div style="clear: both"></div><hr> <small>2023 年 8 月 15 日星期二 06:02:15 GMT+0000（协调世界时间）</small> ]]>;</description><pubDate> Tue, 15 Aug 2023 06:02:15 GMT</pubDate><guid ispermalink="false"> https://twitter.com/_akhaliq/status/1691329433884631040</guid><link/> https://twitter.com/_akhaliq/status/1691329433884631040 <author><![CDATA[AK]]></author></item><item><title><![CDATA[AK: The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation paper page: https://huggingface.co/pa...]]></title><description><![CDATA[<img width="0" height="0" hidden="true" src="https://pbs.twimg.com/media/F3jOiFAbsAASHCK?format=jpg&amp;name=orig" referrerpolicy="no-referrer"> <a href="https://twitter.com/_akhaliq" target="_blank" rel="noopener noreferrer"><img width="48" height="48" src="https://pbs.twimg.com/profile_images/1451191636810092553/kpM5Fe12_normal.jpg" hspace="8" vspace="8" align="left" referrerpolicy="no-referrer"> <strong>AK</strong></a> ：魔鬼就在错误中：利用大型语言模型进行细粒度机器翻译评估<br><br>纸质页面：https://huggingface.co/papers/2308.07286<br><br>机器翻译（MT）的自动评估是推动MT系统快速迭代发展的关键工具。尽管…<br clear="both"><div style="clear: both"></div> <a href="https://pbs.twimg.com/media/F3jOiFAbsAASHCK?format=jpg&amp;name=orig" target="_blank" rel="noopener noreferrer"><img height="150" style="height: 150px;" hspace="4" vspace="8" src="https://pbs.twimg.com/media/F3jOiFAbsAASHCK?format=jpg&amp;name=orig" referrerpolicy="no-referrer"></a><br clear="both"><div style="clear: both"></div><hr> <small>2023 年 8 月 15 日星期二 05:59:35 GMT+0000（协调世界时）</small> ]]>;</description><pubDate> Tue, 15 Aug 2023 05:59:35 GMT</pubDate><guid ispermalink="false"> https://twitter.com/_akhaliq/status/1691328764960284672</guid><link/> https://twitter.com/_akhaliq/status/1691328764960284672 <author><![CDATA[AK]]></author></item><item><title><![CDATA[AK: OctoPack: Instruction Tuning Code Large Language Models paper page: https://huggingface.co/papers/2308.07124 Finetuning large language models (LLM...]]></title><description><![CDATA[<img width="0" height="0" hidden="true" src="https://pbs.twimg.com/media/F3jNhVKbUAALqhq?format=jpg&amp;name=orig" referrerpolicy="no-referrer"> <a href="https://twitter.com/_akhaliq" target="_blank" rel="noopener noreferrer"><img width="48" height="48" src="https://pbs.twimg.com/profile_images/1451191636810092553/kpM5Fe12_normal.jpg" hspace="8" vspace="8" align="left" referrerpolicy="no-referrer"> <strong>AK</strong></a> ：OctoPack：指令调优代码大型语言模型<br><br>纸质页面：https://huggingface.co/papers/2308.07124<br><br>根据指令微调大型语言模型 (LLM) 可显着提高自然语言任务的性能。我们使用代码应用指令调整，利用自然......<br clear="both"><div style="clear: both"></div> <a href="https://pbs.twimg.com/media/F3jNhVKbUAALqhq?format=jpg&amp;name=orig" target="_blank" rel="noopener noreferrer"><img height="150" style="height: 150px;" hspace="4" vspace="8" src="https://pbs.twimg.com/media/F3jNhVKbUAALqhq?format=jpg&amp;name=orig" referrerpolicy="no-referrer"></a><br clear="both"><div style="clear: both"></div><hr> <small>2023 年 8 月 15 日星期二 05:55:10 GMT+0000（协调世界时）</small> ]]>;</description><pubDate> Tue, 15 Aug 2023 05:55:10 GMT</pubDate><guid ispermalink="false"> https://twitter.com/_akhaliq/status/1691327654346272768</guid><link/> https://twitter.com/_akhaliq/status/1691327654346272768<author><![CDATA[AK]]></author></item></channel></rss>